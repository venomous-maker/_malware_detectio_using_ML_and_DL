# -*- coding: utf-8 -*-
"""cnn_fuzz_sec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-fzagwBnRYuxu281-HQRTOpE4Y1ehFKd
"""

import pandas as pd
import numpy as np
import hashlib #for generating hashes
import random
import numpy as np
from sklearn import linear_model
import sklearn.metrics as sm
import matplotlib.pyplot as plt

#importing data from a dataset
#A sample of of cryptojacking malware hashes
file_path = "/content/drive/MyDrive/csv_files/SoK/VT Dataset/VT_miners.csv"
data = pd.read_csv(file_path)

#Checking data stracture
print(data.describe())

#checking sample data
print(data.head())

#checking dataset's length
print(data.count())

#Adding column headers to a dataset to avoid using first hash as a column header
data.columns = ['Number', 'Hash']
print(data.columns)

print(data.head())

print(data.isna().sum().sum(),' are NaN values found')

#hence no NaNs found, no need of cleaning
#Let's generate another dataframe with random hashed data
column1 = [i for i in range(1,101,1)]
# Let's check the data
print(column1[0:10])

#Creating column2 having hashed data with Sha-2
column2 = [random.random() for i in range(1,101)]
print(column2)

#New dictionary
dict1 = {}
dict1['Number'] = column1
dict1['Hash'] = column2
print(dict1)

#Converting the dictionary set into a dataset
data2 = pd.DataFrame(dict1)
print(data2.head())

#Converting Hash from numbers to string
data2['Hash'] = data2['Hash'].astype(str)
# Apply hashing function to the Hash column
data2['Hash'] = data2['Hash'].apply(
    lambda x: 
        hashlib.sha256(x.encode()).hexdigest()
)
#check the change
print(data2.head())

# Attaching CNN model
import tensorflow as tf
from keras import  layers, models

"""SPLITTING DATA INTO TRAIN AND TEST SETS"""

#train data

y_train = data.iloc[:500,0] # column one
x_train = np.array(data.iloc[:500,1]).reshape(-1,1) # column two
x_train_keep = x_train

# test data
y_test = data2.iloc[:,0]
x_test = np.array(data2.iloc[:,1]).reshape(-1,1)
x_test_keep = x_test

#Training using random forest
# Feature Scaling
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import StandardScaler
lableencoder_X_2 = LabelEncoder()
x_train = lableencoder_X_2.fit_transform(x_train)
x_test = lableencoder_X_2.fit_transform(x_test)
x_train = x_train.reshape(-1,1)
x_test = x_test.reshape(-1,1)
# create linear regression object
reg = linear_model.LinearRegression()
  
# train the model using the training sets
reg.fit(x_train, y_train)

y_test_pred = reg.predict(x_test)
for i in range(0,len(y_test_pred)):
  for j in range(0, len(x_train)):
    if int(y_test_pred[i]) == x_train[j]:
      print(x_test_keep[i]," has a close matche to", x_train_keep[j])
      break;

# regression coefficients
print('Coefficients: ', reg.coef_)

# variance score: 1 means perfect prediction
print('Variance score: {}'.format(reg.score(x_test, y_test)))

# plot for residual error
  
## setting plot style
plt.style.use('fivethirtyeight')
  
## plotting residual errors in training data
plt.scatter(reg.predict(x_train), reg.predict(x_train) - y_train,
            color = "green", s = 10, label = 'Train data')
  
## plotting residual errors in test data
plt.scatter(reg.predict(x_test), reg.predict(x_test) - y_test,
            color = "blue", s = 10, label = 'Test data')
  
## plotting line for zero residual error
plt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2)
  
## plotting legend
plt.legend(loc = 'upper right')
  
## plot title
plt.title("Residual errors")
  
## method call for showing the plot
plt.show()